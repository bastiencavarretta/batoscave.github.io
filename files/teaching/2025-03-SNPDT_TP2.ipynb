{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0688bd8",
   "metadata": {},
   "source": [
    "# Entrainement d'un classifieur binaire sur MNIST\n",
    "\n",
    "## Quelques rappels\n",
    "\n",
    "Nous voulons entraîner un réseau de neuronnes à couches denses (ou multi-layer perceptron (MLP)) dont le but est de classifier des nombres écrits à la main suivant q'ils sont plus grand ou plus petit que $4$. \n",
    "\n",
    "Nous prenons pour fonction d'activation $\\sigma(x) = x\\mathbf{1}_{x\\geq 0}$, et appliquons la fonction softmax en sortie de la dernière couche, à la place de $\\sigma$. La fonction de coût considérée est l'entropie croisée $$c(\\hat y,y) = - \\log(\\hat y_{i_y})$$\n",
    "où $y$ est l'étiquette, de la forme $(0,0, \\cdots,0,1,0,\\cdots,0)$ d'une data $x$, et où $i_y$ est l'indice de la valeur $1$.\n",
    "Nous fixerons la dimension des couches à $(784,500,75,2)$. On rappelle la formule du mode forward (évaluation) d'un tel MLP :\n",
    "\n",
    "$$ z_0 = x $$\n",
    "$$ a^{(i)} = W^{(i)}z^{(i-1)}+b^{(i)}, ~~ \\forall i \\in \\{1,2,3 \\}$$\n",
    "$$z^{(i)} = \\sigma(a^{(i)}), ~~ \\forall i \\in \\{1,2 \\}$$\n",
    "$$ \\hat y = \\mathrm{softmax}(a^{(3)})$$\n",
    "$$ \\text{coût} = c(\\hat y,y)$$\n",
    "\n",
    "Nous avons donc : $z^{(0)}\\in \\R^{784}$, $a^{(1)} \\in \\R^500$, $a^{(2)} \\in \\R^{75}$ et $a^{(3)} \\in \\R^2$.\n",
    "\n",
    "\n",
    "Pour $\\theta := (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}, W^{(3))}, b^{(3)})$, on note $g_\\theta(x)$ la quantité $\\hat y$ obtenue par ce calcul direct. C'est le vecteur de prédiction de $x$, représentant un probabilité la probabilité de chaque label pour cette image.\n",
    "\n",
    "\n",
    "Par \"entraîner le réseau\", on entend minimiser la fonction $$\\phi : \\theta \\mapsto \\mathbb{E}_{x,y \\sim \\mathbb{P}}\\left [ c(g_\\theta(x),y)  \\right ]$$ \n",
    "où $\\mathbb P$ est la distribution des données. Nous appliquons pour cela l'algorithme de **descente de gradient stochastique à passage unique**:  \n",
    "- Cela signifie que pour le calcul de $\\theta_{k+1}$, la direction $\\nabla_\\theta c(g_{\\theta_k}(x_k),y_k)$ est considérée comme une bonne approximation de $\\nabla_\\theta \\phi(\\theta_k)$ en moyenne. \n",
    "- Ici le couple $(x_k,y_k)$ désigne une donnée d'entrainement prise au hasard dans `X_train`$\\times$`Y_train`\n",
    "\n",
    "\n",
    "Nous devons donc être capable d'écrire le mode backward du graphe computationel de la fonction $\\theta \\mapsto c(g_{\\theta_k}(x),y)$ à donnée $(x,y)$ fixée.\n",
    "\n",
    "**Remarque :** Pour se simplifier la vie ici :\n",
    "- on ne remélange pas les données entre deux epochs\n",
    "- on prend un pas constant pour le gradient\n",
    "\n",
    "\n",
    "## Questions préliminaires :\n",
    "\n",
    "1) Quelles sont les dimensions des matrices $W^{(i)}$ ? (Nombre de ligne et de colonnes)\n",
    "1) Calculer sur papier la Jacobienne de la fonction softmax.\n",
    "2) En déduire la formule backward sur l'étape softmax de notre classifieur (i.e. de l'étape $a^{(3)} \\mapsto \\hat y$)\n",
    "3) Partant du noeud $\\delta a^{(3)}$, calculer $\\delta z^{(2)}$, $\\delta b^{(3)}$, $\\delta W^{(3)}$.\n",
    "4) En déduire le mode backward du graphe computationnel de la fonction coût.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ee412",
   "metadata": {},
   "source": [
    "# Implémentation \n",
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4530824-aedb-4327-9f6c-3c3fe7ac4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Chargement de MNIST\n",
    "(X_train,Y_train),(X_test,Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'une donnée de MNIST\n",
    "plt.figure()\n",
    "plt.imshow(X_train[27],cmap = \"binary\")\n",
    "plt.colorbar()\n",
    "print(\"Label_27 = \",Y_train[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbef364-282e-4c6b-8a94-9d36be3602b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer les images en vecteurs colonnes\n",
    "(X_train,Y_train),(X_test,Y_test) = mnist.load_data()\n",
    "dim_input = X_train.shape[1]*X_train.shape[2]\n",
    "X_train = X_train.reshape((X_train.shape[0],dim_input)) \n",
    "X_test = X_test.reshape((X_test.shape[0],dim_input))\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "## Shuffle des données\n",
    "np.random.seed(238)\n",
    "nbr_training_data = X_train.shape[0]\n",
    "shuffle_index = np.random.permutation(nbr_training_data)\n",
    "X_train, Y_train = X_train[shuffle_index], Y_train[shuffle_index]\n",
    "plt.imshow(X_train[27].reshape(28,28),cmap = \"binary\")\n",
    "print(\"Label_27 = \",Y_train[27])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d94448",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784,500,75,2]\n",
    "rate = 10**(-3)\n",
    "nbr_epoch = 3\n",
    "width = 0.1 #width of first initialization\n",
    "\n",
    "\n",
    "## Definition de la fonction softmax\n",
    "# def softmax(x):\n",
    "\n",
    "## Initialisation : choisir des poids de l'ordre de grandeur de \"width\"\n",
    "\n",
    "# W1 = ...\n",
    "\n",
    "\n",
    "## Training with single pass SGD\n",
    "\n",
    "k=0\n",
    "print(nbr_training_data*nbr_epoch)\n",
    "while k < nbr_training_data*nbr_epoch :\n",
    "\n",
    "    # récupérer la donnée/label et adapter le label à notre classifieur binaire\n",
    " \n",
    "    \n",
    "    # créer une étiquette adaptée à la classification voulue\n",
    "\n",
    "\n",
    "    # Calcul du mode forward : ATTENTION A LA SHAPE de array. Utiliser np.matmul...\n",
    "\n",
    "    # Calcul du mode backward : Encore plus ATTENTION\n",
    "\n",
    "\n",
    "    # Mise à jour des coefficients par la descente de gradient stochastique\n",
    "\n",
    "    W3 = W3 - rate*dW3\n",
    "    b3 = b3 - rate*db3\n",
    "    W2 = W2 - rate*dW2\n",
    "    b2 = b2-rate*db2\n",
    "    W1 = W1 - rate*dW1\n",
    "    b1 = b1 - rate*db1\n",
    "    k = k +1\n",
    "    # Print de la progression..\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Test du classifieur\n",
    "\n",
    "#definition de la fonction de classification\n",
    "def classification(X):\n",
    "\n",
    "    #return np.argmax(z3)\n",
    "\n",
    "\n",
    "\n",
    "# Calcul du score moyen sur les données de test\n",
    "\n",
    "size_test = X_test.shape[0]\n",
    "success_rate = 0\n",
    "\n",
    "for test in range(1,size_test):\n",
    "    #if .....:\n",
    "        #success_rate += 1/size_test\n",
    "print(\"success rate = \", success_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd0613",
   "metadata": {},
   "source": [
    "## Test \"à la main\" du classifieur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
